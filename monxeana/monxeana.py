
# This Python3 library contains code focussing on the analysis of MonXe stuff.



#######################################
### Imports
#######################################


import numpy as np
from scipy.integrate import odeint
import matplotlib as mpl
import matplotlib.pyplot as plt
import datetime
import pprint
import os
import random
from scipy.optimize import curve_fit
from matplotlib.ticker import AutoMinorLocator
import matplotlib.patches as patches
from PIL import Image, ImageDraw, ImageFont
from fpdf import FPDF
import matplotlib.image as mpimg
from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox
import getpass
import json
from scipy.stats import chi2 # for "Fabian's calculation" of the Poissonian Error





#######################################
### Generic Definitions
#######################################


# username
username = getpass.getuser()


# paths
if username == "daniel":
    pathstring_monxe = "/home/daniel/Desktop/arbeitsstuff/20180705__monxe/"
elif username == "monxe":
    pathstring_monxe = "/home/monxe/Desktop/"
else:
    pathstring_monxe = "./"
pathstring_measurement_folder = pathstring_monxe +"monxe_measurements/"
pathstring_output_relative = "./output/" # this is the folder (within the measurement folder) where the analysis output is stored
pathstring_data = "./data/" # this is the folder where CoMPASS stores the measurement data
pathstring_data_compass = "./data/DAQ/run/RAW/" # this is the folder where CoMPASS stores the measurement data


# filenames
filename_data_csv = "DataR_CH0@DT5781A_840_run.csv" # timestamp (and eventually waveform) data
filename_data_txt = "CH0@DT5781A_840_EspectrumR_run.txt" # adc spectrum data
filename_histogram_png = "histogram" # histogram plot name


# format
color_uni_blue = '#004A9B'
color_uni_red = '#C1002A'
color_monxe_cyan = "#00E8E8" # the cyan-like color that was used within the MonXe logo
color_histogram = "black"
color_histogram_error = color_monxe_cyan
linewidth_histogram_std = 0.8 # the standard linewidth for a histogram plot


# miscellaneous
n_adc_channels = 16383 # from channel 0 to channel 16383
adc_channel_min = 0
adc_channel_max = 16382





#######################################
### Generic Functions
#######################################


# This function is used to convert a datestring (as I defined it, e.g. '20200731') into a format that can be handled by 'datetime'.
def mod_datetimestring(input_string):
    y = input_string[2:4]
    m = input_string[4:6]
    d = input_string[6:8]
    H = input_string[9:11]
    M = input_string[11:13]
    if len(input_string) == 15:
        S = input_string[13:15]
    else:
        S = r"00"

    return d +r"/" +m +r"/" +y +r" " +H +r":" +M +r":" +S


# This function is used to convert a datetime string (as defined by datetime, e.g. '31-07-20 15:31:25') into a datetime object.
def convert_string_to_datetime_object(datetime_str):
    datetime_obj = datetime.datetime.strptime(datetime_str, '%d/%m/%y %H:%M:%S')
    return datetime_obj
#def convert_string_to_datetime_object(datetime_str):
#    datetime_obj = datetime.datetime.strptime(datetime_str, '%Y%m%d_%H%M')
#    return datetime_obj
#convert_string_to_datetime_object(datetime_str="20200731_1530")





#######################################
### Retrieving Data
#######################################


# This is the dtype used for raw data.
timestamp_data_dtype = np.dtype([
    ("timestamp_ps", np.uint64), # timestamp in ps
    ("pulse_height_adc", np.int16) # max adc channel is ~16000, np.int16 ranges from -32768 to 32767
])


# This function is used to load the .csv file that stores the timestamp information and is generated by CoMPASS.
def get_timestamp_data(
    runname = "",
    pathstring_data_input = "",
    flag_extractwaveforms = False,
    flag_daqprogram = ["mc2analyzer", "compass"][1]):

    # retrieving the data pathstring
    if pathstring_data_input == "":
        pathstring_data = pathstring_measurement_folder +runname +"/" +pathstring_data_compass +filename_data_csv
    else:
        pathstring_data = pathstring_data_input
    timestamp_data_tuplelist = []
    waveform_dict = {}

    # opening the .csv file and writing the data into 'timestamp_tuple_list' or 'waveform_dict' respectively
    with open(pathstring_data) as input_file:
        if flag_daqprogram == ["mc2analyzer", "compass"][1]: 
            for line in input_file:
                if not line.startswith("BOA"):
                    line_list = list(line.split(";"))
                    timestamp_ps = np.uint64(line_list[2])
                    pulse_height_adc = np.uint64(line_list[3])
                    # extracting just the timestamp data
                    if flag_extractwaveforms == False:
                        timestamp_data_tuplelist.append((
                            timestamp_ps,
                            pulse_height_adc
                        ))
                    # extracting timestamp plus waveform data
                    elif flag_extractwaveforms == True:
                        waveform_dict.update({
                            str(timestamp_ps) : {
                                "timestamp_ps" : timestamp_ps,
                                "pulse_height_adc" : pulse_height_adc,
                                "waveform" : np.array(line_list[4:])
                            }
                        })
                    else:
                        print(f"'flag_extractwaveforms' invalid: {flag_extractwaveforms}")
        elif flag_daqprogram == ["mc2analyzer", "compass"][0]:
            for line in input_file:
                if not line.startswith("HEADER"):
                    line_list = list(line.split())
                    timestamp_ps = np.uint64(line_list[0])
                    pulse_height_adc = np.int64(line_list[1])
                    # extracting just the timestamp data
                    if pulse_height_adc >= 0:
                        timestamp_data_tuplelist.append((
                            timestamp_ps,
                            pulse_height_adc))
        else:
            raise Exception("invalid 'flag_daqprogram'")

    # returning the requested data
    if flag_extractwaveforms == False or flag_daqprogram == ["mc2analyzer", "compass"][0]:
        return np.array(timestamp_data_tuplelist, timestamp_data_dtype)
    elif flag_extractwaveforms == True:
        return waveform_dict
    else:
        return


# This function is used to infer the duration of a measurement. The timestamps listed by CoMPASS are given in picoseconds.
def get_measurement_duration(
    list_file_data,
    flag_unit = ["days", "minutes", "seconds"][0]
):
    conv_dict = {
        "days" : 24 *60* 60* 1000* 1000* 1000 *1000,
        "minutes" : 60* 60* 1000* 1000* 1000 *1000,
        "seconds" : 60* 1000* 1000* 1000 *1000,
    }
    t_ps = list_file_data[len(list_file_data)-1]["timestamp_ps"]
    return t_ps *(1/conv_dict[flag_unit])


# This function is used to print the
def print_general_measurement_data(
    list_file_data
):

    # 
    print("---------------------------------------")
    print("--- General Measurement Information ---")
    print("---------------------------------------")

    # measurement duration
    t_m_days = get_measurement_duration(list_file_data=list_file_data, flag_unit="days")
    print(f"measurement duration: {t_m_days:.3f} days")

    # recorded events
    print(f"recorded events: {len(list_file_data)} (ch0: {len(list_file_data[(list_file_data['pulse_height_adc'] == 0)])})")

    print("---------------------------------------")
    return





#######################################
### Histogram Stuff
#######################################


# This is the dtype used for histogram data.
histogram_data_dtype = np.dtype([
    ("bin_centers", np.int16), # max adc channel is ~16000, np.int16 ranges from -32768 to 32767
    ("counts", np.uint64), # better safe than sorry
    ("counts_errors", np.uint64) # better safe than sorry
])


# This function is used to calculate the Poissonian error of a number of counts.
def calc_poissonian_error(
    number_of_counts,
    flag_mode = "fabian_symmetrical" # ["sqrt", "fabian", "fabian_symmetrical"]
):
    # for a large number of entries
    if flag_mode == "sqrt":
        if number_of_counts == 0:
            poissonian_error = 1
        else:
            poissonian_error = np.sqrt(number_of_counts)
        return poissonian_error
    # asymmetrical error; use "fabian_symmetrical" for curve_fit
    elif flag_mode in ["fabian", "fabian_symmetrical"]:
        alpha = 0.318
        low, high = (chi2.ppf(alpha/2, 2*number_of_counts) / 2, chi2.ppf(1-alpha/2, 2*number_of_counts + 2) / 2)
        if number_of_counts == 0:
            low = 0.0
        low_interval = number_of_counts - low
        high_interval = high - number_of_counts
        if flag_mode == "fabian":
            return low_interval, high_interval
        elif flag_mode == "fabian_symmetrical":
            return max(low_interval, high_interval)
    # catching exceptions
    else:
        raise Exception("Invalid input: 'flag_mode'.")


# This function is used to convert raw timestamp data into histogram data
def get_histogram_data_from_timestamp_data(
    timestamp_data, # the timestamp data retrieved by 'get_timestamp_data'
    number_of_bins = n_adc_channels # the number of bins, per default every adc channel counts as one bin
):

    # calculating binwidth, bin centers and histogram data
    binwidth = (adc_channel_max-adc_channel_min)/(number_of_bins-1)
    data_histogram_adc_channels = np.arange(adc_channel_min, adc_channel_max +binwidth, binwidth)
    data_histogram_counts = np.histogram(
        a=timestamp_data["pulse_height_adc"],
        bins=number_of_bins,
        range=(adc_channel_min -0.5*binwidth,adc_channel_max +0.5*binwidth)
    )[0]

    # casting the rebinned date into an ndarray
    histogram_data_tuplelist = []
    for i in range(len(data_histogram_adc_channels)):
        histogram_data_tuplelist.append((
            data_histogram_adc_channels[i],
            data_histogram_counts[i],
            calc_poissonian_error(data_histogram_counts[i])
        ))
    histogram_data = np.array(histogram_data_tuplelist, histogram_data_dtype)
    return histogram_data


# This function is used to stepize arbitrary histogram data.
# I.e. it takes two list-like objects representing both the bin centers and also the corresponding counts and calculates two new lists containing both the left and right edges of the bins and two instances of the counts.
def stepize_histogram_data(bincenters, counts, counts_errors="", flag_addfirstandlaststep=True):
    # calculating the binwidth and initializing the lists
    binwidth = bincenters[1]-bincenters[0]
    bincenters_stepized = np.zeros(2*len(bincenters))
    counts_stepized = np.zeros(2*len(counts))
    counts_errors_stepized = np.zeros(2*len(counts))
    # stepizing the data
    for i in range(len(bincenters)):
        bincenters_stepized[2*i] = bincenters[i] -0.5*binwidth
        bincenters_stepized[(2*i)+1] = bincenters[i] +0.5*binwidth
        counts_stepized[2*i] = counts[i]
        counts_stepized[2*i+1] = counts[i]
        if counts_errors != "":
            counts_errors_stepized[2*i] = counts_errors[i]
            counts_errors_stepized[2*i+1] = counts_errors[i]
    # appending a zero to both the beginning and end so the histogram can be plotted even nicer
    bin_centers_stepized_mod = [bincenters_stepized[0]] +list(bincenters_stepized) +[bincenters_stepized[len(bincenters_stepized)-1]]
    counts_stepized_mod = [0] +list(counts_stepized) +[0]

    if flag_addfirstandlaststep==False:
        if counts_errors != "":
            return bincenters_stepized, counts_stepized, counts_errors_stepized
        else:
            return bincenters_stepized, counts_stepized
    else:
        if counts_errors != "":
            return bincenters_stepized, counts_stepized, counts_errors_stepized, bin_centers_stepized_mod, counts_stepized_mod
        else:
            return bincenters_stepized, counts_stepized, bin_centers_stepized_mod, counts_stepized_mod


# This function is used to load the 'documentation.json' file and plot the respective comments (i.e. keys) onto a histogram plot
def annotate_documentation_json(
    annotate_ax, # ax object to be annotated
    filestring_documentation_json = "./documentation.json", # filestring determining which 'documentation.json' file to load
    text_fontsize = 11, # font size of the annotated text
    text_color = "black", # color of the annotated text
    text_x_i = 0.03, # x coordinate of the first text line (relative to the x axis)
    text_y_i = 0.75, # y coordinate of the first text line (relative to the y axis)
    text_parskip = 0.09, # text parskip
    flag_keys = "", # flag determining whether and in which order the keys are to be printed, default is printing all via ""
    flag_addduration = True, # flag determining whether the duration of the measurement is calculated
    flag_print_comment=False, # flag determining whether the 'comment' key should be printed or not
    flag_orientation="left" # flag determining whether the text is printed flushed right or left
):
    ### loading the data from the 'documentation.json' file
    with open(filestring_documentation_json) as json_file:
        doc_data_dict = json.load(json_file)
    ### preparing the text annotation
    ctr_textpos = 0
    # calculating the duration of the measurement
    if flag_addduration == True:
        t_i = datetime.datetime.strptime(doc_data_dict["start"], '%y/%m/%d %H:%M')
        t_f = datetime.datetime.strptime(doc_data_dict["end"], '%y/%m/%d %H:%M')
        t_delta = t_f -t_i
        doc_data_dict.update({"duration" : str(t_delta)})
    
    # determining which keys from 'documentation.json' are being printed onto the plot
    if flag_keys == "":
        keys_iterlist = sorted([*doc_data_dict])
    else:
        keys_iterlist = flag_keys
    ### annotating the comment keys retrieved from the .json file
    for key in keys_iterlist:
        if (key != "comment") or (key == "comment" and flag_print_comment == True):
            plt.text(
                x=text_x_i,
                y=text_y_i -ctr_textpos*text_parskip,
                s=r"\textbf{"+ key +r"}: " +doc_data_dict[key].replace("_","\_"),
                fontsize=text_fontsize,
                color=text_color,
                rotation=0,
                horizontalalignment=flag_orientation,
                verticalalignment='center',
                transform=annotate_ax.transAxes
            )
            ctr_textpos += 1
    return





#######################################
### Fitting
#######################################


# Function to define a gaussian curve with amplitude "A", mean "mu" and sigma "sigma".
def function_gauss(x,A,mu,sigma):
    return A/np.sqrt(2*np.pi*sigma**2)*np.exp(-(x-mu)**2/(2*sigma**2))


# Function to define a Crystall Ball curve.
# See Stefan Bruenner's Thesis (p.30) for more details.
def function_crystal_ball_one(x, mu, sigma, alpha, n, N) -> float: # evtl. N als Parameter
    A = (n/abs(alpha))**n *np.exp(-((abs(alpha)**2)/(2)))
    B = (n/abs(alpha)) -abs(alpha)
    if sigma == 0:
        comp_val = 12
    else:
        comp_val = (float(x)-float(mu))/float(sigma)
    #C = (n/abs(alpha)) *(1/(n-1)) *np.exp(-((abs(alpha)**2)/(2)))
    #D = np.sqrt(math.pi/2) *(1 +erf(abs(alpha)/np.sqrt(2)))
    #N = 1/(sigma*(C+D))
    if comp_val > (-1)*alpha:
        return N * np.exp(-(((x-mu)**2)/(2*sigma**2)))
    if comp_val <= (-1)*alpha:
        return N * A* (B - ((x-mu)/(sigma)))**(-n)

# curve_fit() has problems fitting piecewise defined functions, such as crystal_ball_function().
# Online I found, that one has to vectorize the function in order for curve_fit() to be able to fit it properly.
# Here's the corresponding link I found (accessed 12th March 2019): https://stackoverflow.com/questions/11129812/fitting-piecewise-function-in-python
def function_crystal_ball_one_vec(x, mu, sigma, alpha, n, N):
    y = np.zeros(x.shape)
    for i in range(len(y)):
        y[i]=function_crystal_ball_one(x[i], mu, sigma, alpha, n, N)
    return y


#Function to define a double Crystal Ball curve utilizing the (single) Crystal Ball Curve defined above
def function_crystal_ball_two(x, mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1):
    return function_crystal_ball_one(x, mu_0, sigma_0, alpha_0, n_0, N_0) + function_crystal_ball_one(x, mu_1, sigma_1, alpha_1, n_1, N_1)


# curve_fit() has problems fitting piecewise defined functions, such as quad_crystal_ball_function().
# Online I found, that one has to vectorize the function in order for curve_fit() to be able to fit it properly.
# Here's the corresponding link I found (accessed 12th March 2019): https://stackoverflow.com/questions/11129812/fitting-piecewise-function-in-python
def function_crystal_ball_two_vec(x, mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1):
    y = np.zeros(x.shape)
    for i in range(len(y)):
        y[i]=function_crystal_ball_two(x[i], mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1)
    return y


# Function to define a triple Crystal Ball curve utilizing the (single) Crystal Ball Curve defined above.
def function_crystal_ball_three(x, mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1, mu_2, sigma_2, alpha_2, n_2, N_2):
    return function_crystal_ball_one(x, mu_0, sigma_0, alpha_0, n_0, N_0) +function_crystal_ball_one(x, mu_1, sigma_1, alpha_1, n_1, N_1) +function_crystal_ball_one(x, mu_2, sigma_2, alpha_2, n_2, N_2)


# curve_fit() has problems fitting piecewise defined functions, such as tri_crystal_ball_function().
# Online I found, that one has to vectorize the function in order for curve_fit() to be able to fit it properly.
# Here's the corresponding link I found (accessed 12th March 2019): https://stackoverflow.com/questions/11129812/fitting-piecewise-function-in-python
def function_crystal_ball_three_vec(x, mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1, mu_2, sigma_2, alpha_2, n_2, N_2):
    y = np.zeros(x.shape)
    for i in range(len(y)):
        y[i]=function_crystal_ball_three(x[i], mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1, mu_2, sigma_2, alpha_2, n_2, N_2)
    return y


# Function to define a quadruple Crystal Ball curve utilizing the (single) Crystal Ball Curve defined above.
def function_crystal_ball_four(x, mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1, mu_2, sigma_2, alpha_2, n_2, N_2, mu_3, sigma_3, alpha_3, n_3, N_3):
    return function_crystal_ball_one(x, mu_0, sigma_0, alpha_0, n_0, N_0) +function_crystal_ball_one(x, mu_1, sigma_1, alpha_1, n_1, N_1) +function_crystal_ball_one(x, mu_2, sigma_2, alpha_2, n_2, N_2) +function_crystal_ball_one(x, mu_3, sigma_3, alpha_3, n_3, N_3)


# curve_fit() has problems fitting piecewise defined functions, such as quad_crystal_ball_function().
# Online I found, that one has to vectorize the function in order for curve_fit() to be able to fit it properly.
# Here's the corresponding link I found (accessed 12th March 2019): https://stackoverflow.com/questions/11129812/fitting-piecewise-function-in-python
def function_crystal_ball_four_vec(x, mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1, mu_2, sigma_2, alpha_2, n_2, N_2, mu_3, sigma_3, alpha_3, n_3, N_3):
    y = np.zeros(x.shape)
    for i in range(len(y)):
        y[i]=function_crystal_ball_four(x[i], mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1, mu_2, sigma_2, alpha_2, n_2, N_2, mu_3, sigma_3, alpha_3, n_3, N_3)
    return y


# Function to define a quadruple Crystal Ball curve utilizing the (single) Crystal Ball Curve defined above.
def function_crystal_ball_five(x, mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1, mu_2, sigma_2, alpha_2, n_2, N_2, mu_3, sigma_3, alpha_3, n_3, N_3, mu_4, sigma_4, alpha_4, n_4, N_4):
    return function_crystal_ball_one(x, mu_0, sigma_0, alpha_0, n_0, N_0) +function_crystal_ball_one(x, mu_1, sigma_1, alpha_1, n_1, N_1) +function_crystal_ball_one(x, mu_2, sigma_2, alpha_2, n_2, N_2) +function_crystal_ball_one(x, mu_3, sigma_3, alpha_3, n_3, N_3) +function_crystal_ball_one(x, mu_4, sigma_4, alpha_4, n_4, N_4)


# Vectorization of quint_crystal_ball_function()
def function_crystal_ball_five_vec(x, mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1, mu_2, sigma_2, alpha_2, n_2, N_2, mu_3, sigma_3, alpha_3, n_3, N_3, mu_4, sigma_4, alpha_4, n_4, N_4):
    y = np.zeros(x.shape)
    for i in range(len(y)):
        y[i]=function_crystal_ball_five(x[i], mu_0, sigma_0, alpha_0, n_0, N_0, mu_1, sigma_1, alpha_1, n_1, N_1, mu_2, sigma_2, alpha_2, n_2, N_2, mu_3, sigma_3, alpha_3, n_3, N_3, mu_4, sigma_4, alpha_4, n_4, N_4)
    return y


# This function is used to fit a sum of n Crystal Ball functions to a MonXe histogram.
# The output is then a dictionary containing the determined fit parameters for each peak along with the respective errors.
def fit_range_mult_crystal_ball(
    histogram_data, # a ndarray (with columns '', '' and ''; as generated with XXXX) the Crystal Ball fit is applied to
    n = 2, # number of Crystal Ball peaks (n=2 corresponds to two Crystal Ball peaks)
    fit_range = "", # interval when applying the fit to just an interval of the x data (i.e. bin centers)
    **kwargs # see arguments for scipy.curve_fit (e.g. 'p0' and 'bounds'
):

    ### processing the input
    # restricting the fit to a certain range of bin center values
    if fit_range != "":
        fit_data = histogram_data[(histogram_data["bin_centers"] >= fit_range[0]) & (histogram_data["bin_centers"] <= fit_range[1])]
    else:
        fit_data = histogram_data
    # selecting the corresponding fit function
    if n == 1:
        fit_function = function_crystal_ball_one_vec
    elif n==2:
        fit_function = function_crystal_ball_two_vec
    elif n==3:
        fit_function = function_crystal_ball_three_vec
    elif n==4:
        fit_function = function_crystal_ball_four_vec
    elif n==5:
        fit_function = function_crystal_ball_five_vec
    else:
        print("The current implementation of 'fit_range_mult_crystal_ball' only supports a maximum of five Crystal Ball peaks.")
        return

    ### fitting 'fit_data' with 'n' Crystal Ball functions
    # curve_fit output: 
    p_opt, p_cov = curve_fit(
        f = fit_function,
        xdata = fit_data["bin_centers"],
        ydata = fit_data["counts"],
        sigma = fit_data["counts_errors"],
        absolute_sigma = True,
        method='lm', # "lm" cannot handle covariance matrices with deficient rank
        **kwargs
    )
    # calculating the errors of the fit parameters
    p_err = np.sqrt(np.diag(p_cov))

    ### filling the output dictionary with the fit parameters
    fit_parameter_dictionary = {}
    name_parameter = ["mu", "sigma", "alpha", "n", "N"]
    for i in range(n):
        fit_parameter_dictionary.update({str(i) : {}})
        fit_parameter_dictionary[str(i)].update({"fit_data" : {}})
        fit_parameter_dictionary[str(i)].update({"fit_data_errors" : {}})
        for j in range(5):
            fit_parameter_dictionary[str(i)]["fit_data"].update({name_parameter[j] : p_opt[(i*5)+j]})
            fit_parameter_dictionary[str(i)]["fit_data_errors"].update({name_parameter[j] : p_err[(i*5)+j]})
    return fit_parameter_dictionary



# This function is used to generate a ndarray (with columns 'x' and 'y') which can be used to plot a 2D plot
def get_function_values_for_plotting(
    function, # function which is used to calculate the y values
    x_min, # minimum x value
    x_max, # maximum x value
    n_samples, # number of samples
    **kwargs # keyword arguments which are passed on to the function call (e.g. parameters for Crystal Ball functions)
):
    # defining the ndarray dtype
    gnampf_dtype = np.dtype([
        ("x", np.float64),
        ("y", np.float64)
    ])
    # generating data and saving the ndarray
    tuple_list = [(x, function(x, **kwargs)) for x in np.linspace(start=x_min, stop=x_max, num=n_samples, endpoint=True)]
    data = np.array(tuple_list, gnampf_dtype)
    return data
    

# This function is used to calculate the resolution from the fit parameters for one specific Crystal Ball fit    
def get_resolution(
    single_cb_fit_param_dict,
    single_cb_fit_param_error_dict,
    flag_percent = True # flag determining whether the output is given in percent or in absolute numbers
):
    if flag_percent == True:
        fac = 100
    else:
        fac = 1
    f = 2*np.sqrt(2*np.log(2)) # constant conversion factor for the conversion from a gaussian sigma to the FWHM
    resolution = fac *f *single_cb_fit_param_dict["sigma"]/single_cb_fit_param_dict["mu"]
    resolution_error = fac *np.sqrt( (f *(1/single_cb_fit_param_dict["mu"]) *single_cb_fit_param_error_dict["sigma"])**2  +  (f *single_cb_fit_param_dict["sigma"] *(1/single_cb_fit_param_dict["mu"]**2) *single_cb_fit_param_error_dict["mu"])**2 )
    return resolution, resolution_error


# This function is used to add plottable graph data to the 'fit_parameter_dictionary' generated by 'fit_range_mult_crystal_ball'.
def add_graph_data_to_fpd(fit_parameter_dictionary):
    # looping over the peak numbers and adding graph data by calling 'get_function_values_for_plotting' 
    for key in fit_parameter_dictionary:
        fit_parameter_dictionary[key].update({"graph_data" : get_function_values_for_plotting(function=function_crystal_ball_one, x_min=0, x_max=adc_channel_max, n_samples=4000, **fit_parameter_dictionary[key]["fit_data"])})
    return


# This function is used to automatically add peak specific data to the 'fit_parameter_dictionary' generated by 'fit_range_mult_crystal_ball'.
def calc_peak_data(
    peak_data_dictionary,
    timestamp_data_ndarray,
    n_sigma_left = 12,
    n_sigma_right = 3
):
    # energy resolution
    for key in peak_data_dictionary:
        peak_data_dictionary[key].update({"resolution" : {}})
    for key in peak_data_dictionary:
        res, res_err = get_resolution(
            single_cb_fit_param_dict = peak_data_dictionary[key]["fit_data"],
            single_cb_fit_param_error_dict = peak_data_dictionary[key]["fit_data_errors"])
        peak_data_dictionary[key]["resolution"].update({"resolution_in_percent" : res})
        peak_data_dictionary[key]["resolution"].update({"resolution_error" : res_err})
    # counts
    for key in peak_data_dictionary:
        peak_data_dictionary[key].update({"counts" : {}})
    for key in peak_data_dictionary:
        if "left_border_adx" in peak_data_dictionary[key]["isotope_data"] and "right_border_adx" in peak_data_dictionary[key]["isotope_data"]:
            left_border_adc = peak_data_dictionary[key]["isotope_data"]["left_border_adc"]
            right_border_adc = peak_data_dictionary[key]["isotope_data"]["right_border_adc"]
        else:
            left_border_adc = peak_data_dictionary[key]["fit_data"]["mu"] -n_sigma_left*peak_data_dictionary[key]["fit_data"]["sigma"]
            right_border_adc = peak_data_dictionary[key]["fit_data"]["mu"] +n_sigma_right*peak_data_dictionary[key]["fit_data"]["sigma"]
        peak_data_dictionary[key]["counts"].update({
            "left_border_adc" : left_border_adc,
            "right_border_adc" : right_border_adc,
            "counts" : len(timestamp_data_ndarray[(timestamp_data_ndarray["pulse_height_adc"]>=left_border_adc) & (timestamp_data_ndarray["pulse_height_adc"]<=right_border_adc)])
        })
    # end of function
    return


# This function is used to print the contents of 'peak_parameter_dict'
def print_peak_data_dict_contents(pdd):
    for k in pdd:
        print("\n########################################")
        print(f"peak: {k}")
        print("########################################\n")
        spacingstr = "   "
        for ke in pdd[k]:
            print(ke)
            if type(pdd[k][ke]) == dict:
                for key in pdd[k][ke]:
                    print(f"{spacingstr}{key} : {pdd[k][ke][key]}")
            elif type(pdd[k][ke]) == np.ndarray:
                print(f"{spacingstr}np.ndarray of length {len(pdd[k][ke])}")
            else:
                print(f"{spacingstr}{pdd[k][ke]}")
            print("")
        print("\n")
    return


